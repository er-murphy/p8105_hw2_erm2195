---
title: "P8105 Homework 2"
output: github_document
date: "2023-10-02"
---

# Problem 1
Goal: merging the FiveThirtyEight `pols-month.csv`, `unemployment.csv`, and `snp.csv` data into a single data frame using year and month as keys across datasets.

## Loading Libraries
First, loading the `tidyverse` and `readxl` libraries for use throughout the homework:

```{r library}
library(tidyverse)
library(readxl)
```

## Cleaning `pols-month` Data
Now, cleaning the data in `pols-month.csv`:

* Using separate() to break up the variable `mon` into integer variables `year`, `month`, and `day`
* Replacing month number with month name
* Creating a `president` variable taking values "gop" and "dem"
* Removing `prez_dem` and `prez_gop` variables
* Removing the `day` variable

```{r cleaning_pols}

pols = 
  read_csv("Data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    president = if_else(prez_dem != 0 , "dem", "gop"),
    month = case_match(month,
      1 ~ "January",
      2 ~ "February",
      3 ~ "March",
      4 ~ "April",
      5 ~ "May",
      6 ~ "June",
      7 ~ "July",
      8 ~ "August",
      9 ~ "September",
      10 ~ "October",
      11 ~ "November",
      12 ~ "December")
  ) |> 
  select(-starts_with("prez"), -day)

```

## Cleaning `snp` Data
Second, cleaning the data in `snp.csv` using a similar process:

* Using separate() to break up the variable `date` into integer variables `year`, `month`, and `day`
* Converting the `year` variable into a 4 digit value
* Replacing month number with month name
* Reorganizing so that year and month are the leading columns
* Removing the `day` variable

```{r cleaning_snp}
snp = 
  read_csv("Data/snp.csv") |> 
  janitor::clean_names() |> 
  separate(date, into = c("month", "day", "year"), sep = "/") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    year = if_else(as.integer(year) < 24, as.integer(year) + 2000, as.integer(year) + 1900),
    month = case_match(month,
      1 ~ "January",
      2 ~ "February",
      3 ~ "March",
      4 ~ "April",
      5 ~ "May",
      6 ~ "June",
      7 ~ "July",
      8 ~ "August",
      9 ~ "September",
      10 ~ "October",
      11 ~ "November",
      12 ~ "December")
  ) |> 
  select(year, month, close)

```

## Cleaning `unemployment` Data
Third, tidying the data in `unemployment.csv` so that it can be merged with the other two datasets:

* Switch from "wide" to "long" format - turn months into a single column, and the value of existing `months` column into a column called `unemployment`
* Converting month names to capitalized full names to match the format of other datasets

```{r cleaning_unemployment}
unemployment = 
  read_csv("Data/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(jan:dec, names_to = "month", values_to = "unemployment") |> 
  mutate(
    month = case_match(month,
    "jan" ~ "January",
    "feb" ~ "February",
    "mar" ~ "March",
    "apr" ~ "April",
    "may" ~ "May",
    "jun" ~ "June",
    "jul" ~ "July",
    "aug" ~ "August",
    "sep" ~ "September",
    "oct" ~ "October",
    "nov" ~ "November",
    "dec" ~ "December")
  )
```

## Merging the Data Sets
Joining the datasets by merging `snp` into `pols`, and then merging `unemployment` into the result. The resulting dataset is called `data_538`.

```{r merging_pols_snp_unemployment}

data_538 = left_join(pols, snp) |> 
  left_join(x = _, y = unemployment)

str(data_538)
```

## Description of the Combined 538 Data Set
The `pols` data set has `r nrow(pols)` observations and `r ncol(pols)` variables. It details the party affiliation distribution (democrat or republican) for governors and senators in a given year from `r pols |> pull(year) |> min()` to `r pols |> pull(year) |> max()`. It also specifies the political party of the president at that time. The `snp` data has `r nrow(snp)` observations and `r ncol(snp)` variables, and contains the closing value of the S&P index on the associated date. The included dates range from `r snp |> pull(year) |> min()` to `r snp |> pull(year) |> max()`. The `unemployment` data has `r nrow(unemployment)` observations and `r ncol(unemployment)` variables, and contains the percentage of unemployment on the associated date, ranging from years `r unemployment |> pull(year) |> min()` to `r unemployment |> pull(year) |> max()`. 

From the merged `538_data` data set we can see that in January from 1975 onward, in years with a democratic president, the average unemployment rate was `r filter(data_538, month == "January", year >= 1975, president == "dem") |> pull(unemployment) |> mean() |> round(2)`. For comparison, the average unemployment rate over the same time period in which a republican was president was `r filter(data_538, month == "January", year >= 1975, president == "gop") |> pull(unemployment) |> mean() |> round(2)`.


# Problem 2

## Cleaning `Mr. Trash Wheel` Data
First, reading in and cleaning the data from the Excel file sheet `Mr. Trash Wheel`:

* Specifying the sheet in the Excel file
* Using arguments in `read_excel`, omitting the first row and `Homes` column, given that they contain notes and Excel-based functions
* Removing the last row, which is a sum count of column values and not dumpster-specific data, as well as any rows with no value for the `dumpster` value
* Converting the year variable to be of numeric type, to better match with the other two datasets coming up
* Fixing an issue where one row specifies a month and year of January 2020, but the associated date is 1/20/1900 rather than 1/20/2020
* Creating a `homes_powered` for the approximate number of homes powered, based on the calculation described in the Homes powered note. Rounding this a whole number, which makes more intuitive sense than powering 0.27 of a home
* Rounding the `sports_balls` column so that it's in whole numbers like the other trash count variables

Note: Rather than specifying the columns to import in the read_excel statement, I think it makes more sense to import them all and then just drop the Homes column specifically. That way, this code will still work if additional columns are added in future years. But, doing it this way as it seems like the right call, given that the question specified that we should omit columns with non-data entries within the arguments of `read_excel`.

```{r cleaning_mister}
mister = 
  read_excel("Data/202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = cell_cols("A:M"), skip = 1, col_names = TRUE) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  arrange(date) |> 
  mutate(
    year = as.integer(year),
    date = if_else(row_number() == 1, as_date("2020-01-20"), date),
    homes_powered = round((weight_tons * 500) / 30, digits = 0),
    sports_balls = round(sports_balls, digits = 0)
  )
```

## Cleaning `Professor Trash Wheel` Data
Next, similarly reading in and cleaning the data from the Excel file sheet `Professor Trash Wheel`:

* Specifying the sheet in the Excel file
* Using arguments in `read_excel`, omitting non-data entries like rows with figures and columns containing notes. As with the Mr. Trash Data, this is the first row and the `Homes` column
* Removing the last row, which is a sum count of column values and not dumpster-specific data, as well as any rows with no value for the `dumpster` value
* Creating a `homes_powered` for the approximate (whole) number of homes powered, based on the calculation described in the Homes powered note

```{r cleaning_professor}
professor = 
  read_excel("Data/202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = cell_cols("A:L"), skip = 1, col_names = TRUE) |>
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    homes_powered = round((weight_tons * 500) / 30, digits = 0)
  )
```

## Cleaning `Gwynnda Trash Wheel` Data
Next, similarly reading in and cleaning the data from the Excel file sheet `Gwynnda Trash Wheel`:

* Specifying the sheet in the Excel file
* Using arguments in `read_excel`, omitting non-data entries like rows with figures and columns
* Removing the last row, which is a sum count of column values and not dumpster-specific data, as well as any rows with no value for the `dumpster` value
* Creating a `homes_powered` for the approximate (whole) number of homes powered, based on the calculation described in the Homes powered note

```{r cleaning_gywnnda}
gwynnda = 
  read_excel("Data/202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range = cell_cols("A:L"), skip = 1, col_names = TRUE) |>
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(
    homes_powered = round((weight_tons * 500) / 30, digits = 0)
  )
```

## Binding the Datasets

Now, adding a factor column to each dataset to identify the name of the source water-wheel (to help with identification post-merge). Then, binding the `professor` and `gwynnda` datasets together with the `mister` dataset to produce a single dataset. Finally, organizing the combined dataset to make it easier to understand by sorting it chronologically and re-arranging columns so that `sports_balls` is alongside the other types of trash, and `water_wheel` is moved to a more prominent location.

```{r joining_trash_data}
mister = mutate(mister, water_wheel = "Mr. Trash Wheel")
professor = mutate(professor, water_wheel = "Professor Trash Wheel")
gwynnda = mutate(gwynnda, water_wheel = "Gwynnda Trash Wheel")

trash_wheels_tidy = 
  bind_rows(professor, gwynnda, mister) |> 
  mutate(
    water_wheel = as.factor(water_wheel)
  ) |> 
  arrange(date) |> 
  select(dumpster, water_wheel, month:volume_cubic_yards, homes_powered, everything())

```

## Description of the Combined Trash Wheels Data Set
The `trash_wheels_tidy` data set has `r nrow(trash_wheels_tidy)` observations and `r ncol(trash_wheels_tidy)` variables. It details the contents of dumpsters of trash collected by `r nlevels(pull(trash_wheels_tidy, var = water_wheel))` water-wheel vessels in Baltimore's Inner Harbor on specific dates between `r trash_wheels_tidy |> pull(year) |> min()` and `r trash_wheels_tidy |> pull(year) |> max()`. Of the `r nlevels(pull(trash_wheels_tidy, var = water_wheel))` wheels, the greatest amount of trash collected by a single wheel is `r filter(trash_wheels_tidy, water_wheel == "Mr. Trash Wheel") |> pull(weight_tons) |> sum()` tons by Mr. Trash Wheel. In comparison, Professor Trash Wheel has only removed `r filter(trash_wheels_tidy, water_wheel == "Professor Trash Wheel") |> pull(weight_tons) |> sum()` total tons of trash from the harbor.

In addition to the total weight and volume of trash collected, the dataset also includes counts of commonly-encountered debris. For example, in July 2021, Gwynnda Trash Wheel encountered `r format(filter(trash_wheels_tidy, water_wheel == "Gwynnda Trash Wheel", year == 2021, month == "July" ) |> pull(cigarette_butts) |> sum(), scientific = FALSE)` total cigarette butts. The good news is that those butts, in combination with the other trash she collected that month, is enough to power `r filter(trash_wheels_tidy, water_wheel == "Gwynnda Trash Wheel", year == 2021, month == "July" ) |> pull(homes_powered) |> sum()` homes.

# Problem 3

## Background and Data Collection
This problem uses data collected in an observational study to understand the trajectory of Alzheimer’s disease (AD) biomarkers. Study participants were free of Mild Cognitive Impairment (MCI), a stage between the expected cognitive decline of normal aging and the more serious decline of dementia, at the study baseline.

Basic demographic information were measured at the study baseline. The study monitored the development of MCI and recorded the age of MCI onset during the follow-up period, with the last visit marking the end of follow-up. APOE4 is a variant of the apolipoprotein E gene, significantly associated with a higher risk of developing Alzheimer’s disease. The amyloid β
 42/40 ratio holds significant promise for diagnosing and predicting disease outcomes. This ratio undergoes changes over time and has been linked to the manifestation of clinical symptoms of Alzheimer’s disease.

## Cleaning `Baseline` Data
Importing, cleaning, and tidying the dataset of baseline demographics:

* Skipping the first line of the CSV, which detail the codes for each variable
* Renaming the `age_at_baseline` and `years_education` variables to more clearly describe their contents
* Converting the `id` and `years_education` variables to integers
* Converting the `age_at_baseline` and `age_at_onset` variables to numeric (not integers in order to preserve the months details)
* Converting the `sex` and `apoe4` variables to factors based on the described codes for their values
* Filtering out observations where the participant's age of MCI onset is younger than age at baseline, as these individuals have MCI that pre-dates the start of the study (and therefore don't meet the inclusion criteria)

```{r cleaning_baseline}
baseline = 
  read_csv("Data/MCI_baseline.csv", skip = 1, na = ".") |> 
  janitor::clean_names() |> 
  rename(age_at_baseline = current_age, years_education = education) |> 
  mutate (
    id = as.integer(id),
    age_at_baseline = as.numeric(age_at_baseline),
    sex = as.factor(
      case_match(sex,
        1 ~ "male", 
        0 ~ "female")),
    years_education = as.integer(years_education),
    apoe4 = as.factor(
      case_match(apoe4,
        1 ~ "carrier", 
        0 ~ "non-carrier")),  
    age_at_onset = as.numeric(age_at_onset)
  ) |> 
  filter((age_at_onset >= age_at_baseline) | is.na(age_at_onset))

str(baseline)
```

## Description of Study Inclusion Criteria and the Baseline Data Set
The import process for the `baseline` data set included reading in the `MCI_baseline` csv file, renaming the `age` and `education` variables, and converting multiple variables into other types that were a better fit for their contents (for example, changing `sex` to a 2-level factor variable, rather than a character variable). Finally, individuals who were diagnosed with MCI at an earlier age than they had their baseline study visit were excluded, as their condition pre-dates the study period and therefore they don't meet eligibility criteria.

From the resulting cleaned `baseline` data set we can see that `r nrow(baseline)` individuals free of Mild Cognitive Impairment (MCI) were recruited for this study of the trajectory of Alzheimer’s disease (AD) biomarkers. Of this `r nrow(baseline)`, `r filter(baseline, is.na(age_at_onset) == FALSE) |> nrow()` individuals developed MCI over the course of follow-up.The dataset contains `r (ncol(baseline))` columns of demographic information: `r names(baseline)`. The apoe4 variabel refers to a participant's carrier status for the APOE4 variant of the apolipoprotein E gene. The average age at study enrollment was `r baseline |> pull(age_at_baseline) |> mean() |> round(0)` years old, while the average age at MCI onset was `r filter(baseline, is.na(age_at_onset) == FALSE) |> pull(age_at_onset) |> mean() |> round(0)` years old. Among women, `r (filter(baseline, sex == "female", apoe4 == "carrier") |> nrow()) / (filter(baseline, sex == "female") |> nrow()) * 100 |> round(digits = 2)` percent were APOE4 carriers. 

## Cleaning `Baseline` Data
Similarly, importing, cleaning, and tidying the longitudinal dataset of biomarker values:

* Skipping the first line of the CSV, which detail the codes for each variable
* Renaming all variables to more clearly describe their contents and better match the baseline dataset
* Switching from "wide" to "long" format - turning `visits` into a single column, and the corresponding values into a column called `amyloid_beta`
* Converting the amyloid follow-up values to numeric values rounded to 7 decimal places, as this is the highest level of specificity common to all of the amyloid columns

```{r cleaning_follow_up}
follow_up = 
  read_csv("Data/mci_amyloid.csv", skip = 1, na = "NA") |> 
  janitor::clean_names() |> 
  rename(id = study_id, year_0 = baseline, year_2 = time_2, year_4 = time_4, year_6 = time_6, year_8 = time_8) |> 
  pivot_longer(starts_with("year"), names_to = "visit", values_to = "amyloid_beta") |> 
  mutate(
    amyloid_beta = as.numeric(amyloid_beta) |> 
      round(7)
  )

str(follow_up)
```

## Description of Study Inclusion Criteria and the Baseline Data Set
The import process for the `follow_up` data set included reading in the `mci_amyloid` csv file, renaming all of the variables to clarify that they contain values from biennual follow-up visits, and converting the values from said follow-up visits into true numbers rather than characters. Unlike the `baseline` data set, we don't need to worry about excluding participants with pre-existing MCI here, as this data set doesn't contain any details about the outcome, just the exposure (the amyloid beta biomarker).

From the resulting cleaned `follow_up` data set we can see that `r filter(follow_up, is.na(amyloid_beta) == FALSE) |> nrow()` total measurements of participants' amyloid beta 42/40 ratios were collected. Among all participants, the mean amyloid beta 42/40 ratio was `r follow_up |> pull(amyloid_beta) |> mean(na.rm = TRUE) |> round(4)`, while the lowest measurement was `r follow_up |> pull(amyloid_beta) |> min(na.rm = TRUE) |> round(4)` and the highest was `r follow_up |> pull(amyloid_beta) |> max(na.rm = TRUE) |> round(4)`.

## Checking on Presence in Multiple Datasets
It doesn't look like all IDs are present in both the `baseline` and `follow_up` data sets. These participants are only in `baseline`: `r setdiff(pull(baseline, var = "id"), pull(follow_up, var = "id"))`; these participants are only in `follow_up`: `r setdiff(pull(follow_up, var = "id"), pull(baseline, var = "id"))`.

## Checking on Presence in Multiple Datasets
Combining the `baseline` and `follow-up` datasets so that only participants who appear in both datasets are retained, and saving the result as the `mci_biomarkers` dataset:

```{r}
mci_biomarkers = inner_join(baseline, follow_up, by = "id")

str(mci_biomarkers)
```

## Describing the Combined Dataset
The `mci_biomarkers` dataset contains both demographic information and follow-up visit biomarker measurements for `r pull(mci_biomarkers, var = "id") |> unique() |> length()` individuals. It has a total of `r nrow(mci_biomarkers)` observations and contains `r ncol(mci_biomarkers)` variables. On average, each participant had `r ((filter(follow_up, is.na(amyloid_beta) == FALSE) |> nrow()) / (pull(mci_biomarkers, var = "id") |> unique() |> length())) |> round(2)` measurements taken of their amyloid beta 42/40 ratio. 

## Exporting the `mci_biomarkers` Dataset as a CSV
Exporting the `mci_biomarkers` dataset as a CSV to my Homework 3 Data folder:

```{r}
write_csv(mci_biomarkers, "Data/mci_biomarkers.csv", append = FALSE, col_names = TRUE)
```
